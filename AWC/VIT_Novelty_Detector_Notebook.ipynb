{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Classifier + Novelty Detector with Masked Input and Diffusion-Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "07365a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for deep learning, vision models, and utilities.\n",
    "!pip install torch torchvision timm transformers diffusers opencv-python scikit-learn matplotlib albumentations umap-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a074dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries for modeling, image handling, and metrics.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "01f8a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define masking function to simulate occluded images using a donut-style mask.\n",
    "\n",
    "def apply_donut_mask(image, inner_ratio=0.25, outer_ratio=0.75):\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.circle(mask, (w // 2, h // 2), int(w * outer_ratio / 2), 255, -1)\n",
    "    cv2.circle(mask, (w // 2, h // 2), int(w * inner_ratio / 2), 0, -1)\n",
    "    return cv2.bitwise_and(image, image, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "7cc10d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Stable Diffusion model for augmenting masked images with generative realism.\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "def generate_diffused_image(image_pil, prompt=\"a tennis ball\", strength=0.75, guidance_scale=7.5):\n",
    "    image = image_pil.resize((512, 512))\n",
    "    result = pipe(prompt=prompt, image=image, strength=strength, guidance_scale=guidance_scale)\n",
    "    return result.images[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92efeb9",
   "metadata": {},
   "source": [
    "## Step 4: Generate and Save Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "4a0a93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate masked images and produce their diffused versions for training a robust model.\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "raw_folder = \"data/raw/\"\n",
    "masked_folder = \"data/masked/\"\n",
    "diffused_folder = \"data/diffused/\"\n",
    "\n",
    "os.makedirs(masked_folder, exist_ok=True)\n",
    "os.makedirs(diffused_folder, exist_ok=True)\n",
    "\n",
    "image_paths = glob(f\"{raw_folder}/*.jpg\")\n",
    "\n",
    "for path in tqdm(image_paths):\n",
    "    img = cv2.imread(path)\n",
    "    masked = apply_donut_mask(img)\n",
    "    name = os.path.basename(path)\n",
    "    cv2.imwrite(f\"{masked_folder}/{name}\", masked)\n",
    "\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(masked, cv2.COLOR_BGR2RGB))\n",
    "    generated = generate_diffused_image(pil_img, prompt=\"a tennis ball\")\n",
    "    generated.save(f\"{diffused_folder}/{name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17da5d",
   "metadata": {},
   "source": [
    "## Step 5: Load Data for ViT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "e1643e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch dataset class and data loader using the diffused dataset.\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder, labels, transform):\n",
    "        self.paths = glob(folder + \"/*.jpg\")\n",
    "        self.labels = [labels[os.path.basename(p)] for p in self.paths]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(img), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "labels = {os.path.basename(p): 0 for p in image_paths}  # Dummy labels\n",
    "transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "dataset = CustomImageDataset(diffused_folder, labels, transform)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9a590",
   "metadata": {},
   "source": [
    "## Step 6: Train ViT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "ca4ca5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune a pre-trained Vision Transformer on the augmented dataset.\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=3)\n",
    "model = model.to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(\"cuda\"), torch.tensor(labels).to(\"cuda\")\n",
    "        outputs = model(imgs).logits\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec003d",
   "metadata": {},
   "source": [
    "## Step 7: Novelty Detection Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "c27531ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and detect novelty by measuring classification confidence against a threshold.\n",
    "\n",
    "def predict_with_novelty(image, threshold=0.6):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = transform(image).unsqueeze(0).to(\"cuda\")\n",
    "        logits = model(inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        top_prob = np.max(probs)\n",
    "        if top_prob < threshold:\n",
    "            return \"Novel\", top_prob\n",
    "        else:\n",
    "            return f\"Class {np.argmax(probs)}\", top_prob\n",
    "\n",
    "test_img = Image.open(\"some_test_image.jpg\")\n",
    "pred, conf = predict_with_novelty(test_img)\n",
    "print(f\"Prediction: {pred}, Confidence: {conf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc50b90",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "30fc8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned embedding space using t-SNE for interpretability.\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "embeddings, labels_list = [], []\n",
    "model.eval()\n",
    "for imgs, lbls in loader:\n",
    "    imgs = imgs.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        feats = model.vit(imgs).last_hidden_state[:, 0, :]  # CLS token\n",
    "        embeddings.append(feats.cpu().numpy())\n",
    "        labels_list += lbls\n",
    "embeddings = np.concatenate(embeddings)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "proj = tsne.fit_transform(embeddings)\n",
    "\n",
    "sns.scatterplot(x=proj[:, 0], y=proj[:, 1], hue=labels_list)\n",
    "plt.title(\"ViT Embedding Space (t-SNE)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
